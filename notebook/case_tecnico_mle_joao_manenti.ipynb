{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgR1Cxuzr7QN",
        "outputId": "8d6158fc-ece3-4273-ba80-5ae77044a58d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFR1hiskGBkK",
        "outputId": "3e0d595b-95ac-4f0f-e615-da126a32c788"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting airportsdata\n",
            "  Downloading airportsdata-20241001-py3-none-any.whl.metadata (8.9 kB)\n",
            "Downloading airportsdata-20241001-py3-none-any.whl (912 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/912.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/912.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m737.3/912.7 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m912.7/912.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: airportsdata\n",
            "Successfully installed airportsdata-20241001\n"
          ]
        }
      ],
      "source": [
        "#Install Apache Spark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://apache.osuosl.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.3-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "!pip install -U airportsdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqvZyCfnG_jX",
        "outputId": "f50e3ff6-4469-4f49-f453-4631d2a7f8d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Import Apache Spark\n",
        "import os\n",
        "import requests\n",
        "import logging\n",
        "\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.3-bin-hadoop3\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Colab\").getOrCreate()\n",
        "\n",
        "# Try Spark Version\n",
        "spark.range(5).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3wgqjga1Bff"
      },
      "source": [
        "<h1> <b>Data Ingestion with PySpark on Google Colab for the ETL Pipeline</b> </h1>\n",
        "In this phase of our ETL pipeline, we focus on <strong>data ingestion</strong> using PySpark within the Google Colab environment. The goal of this step is to load the flight dataset, which will be used for further analysis and modeling, ensuring that the information is available in the appropriate format for processing and enrichment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6UEVTCBkIFZX"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder \\\n",
        "  .appName(\"FlightDelayAnalysis\") \\\n",
        "  .config(\"spark.executor.memory\", \"4g\") \\\n",
        "  .config(\"spark.driver.memory\", \"4g\") \\\n",
        "  .config(\"spark.executor.cores\", \"2\") \\\n",
        "  .getOrCreate()\n",
        "\n",
        "def ingest_fligths_data(file_path):\n",
        "\n",
        "  df_flights = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "  df_flights.show(5)\n",
        "\n",
        "  return df_flights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amJz1_Ug11jg"
      },
      "source": [
        "# **ENRICHMENT:** Getting ICAO Code from Python Database airportsdata\n",
        "\n",
        "<p>In this part of the ETL process, we enrich our dataset by retrieving the <strong>ICAO codes for airports</strong> using the airportsdata Python library. This enrichment step is crucial for standardizing airport codes and ensuring consistency across our dataset.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dTin1VfFceHZ"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import FloatType, StringType\n",
        "import airportsdata\n",
        "\n",
        "# Call External API AirportDB\n",
        "def get_airport_icao_code(airport_code):\n",
        "  airports = airportsdata.load('IATA')  # key is the IATA location code\n",
        "  return airports[airport_code]['icao']\n",
        "\n",
        "# UDF Functions for Spark\n",
        "\n",
        "get_airport_icao_code_udf = udf(get_airport_icao_code, StringType())\n",
        "\n",
        "def enrich_with_airport_icao_data(df):\n",
        "    df_enriched = df \\\n",
        "        .withColumn('icao_origin', get_airport_icao_code_udf(df['origin'])) \\\n",
        "        .withColumn(\"icao_dest\", get_airport_icao_code_udf(df[\"dest\"]))\n",
        "    return df_enriched"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm3FcIQpy7DU"
      },
      "source": [
        "# **ENRICHMENT:** Getting day of the week by data\n",
        "\n",
        "<p>In this part of the ETL process, we enrich our dataset by retrieving the <strong>day of the week</strong> by using the datetime Python library. This enrichment step is crucial for using necessary APIs services and ensuring consistency across our dataset.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JXt7QOxEzMRH"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "import datetime\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Concatenate Date\n",
        "def get_week_date(year, month, day):\n",
        "    try:\n",
        "        date = datetime.datetime(year, month, day)\n",
        "        day_of_week_name = date.strftime(\"%A\")  # Get the day of the week as a string\n",
        "        logger.debug(f\"Day of the week for {year}-{month}-{day}: {day_of_week_name}\")\n",
        "\n",
        "        return day_of_week_name\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error concatenating date: {e}\")\n",
        "        return None\n",
        "\n",
        "# UDF Functions for Spark\n",
        "get_week_date_udf = udf(get_week_date, StringType())\n",
        "\n",
        "def enrich_get_week_date(df):\n",
        "    df_enriched = df.withColumn('week_date', get_week_date_udf(df['year'], df['month'], df['day']))\n",
        "    return df_enriched"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hbTEFz620Y4"
      },
      "source": [
        "# **ENRICHMENT:** Concatenating Date String\n",
        "Process of enriching date information for integration with the Weatherbit API. The goal is to concatenate date data into a format that is compatible with Weatherbit API.\n",
        "\n",
        "### 1. **Date Formatting**\n",
        "   - **Input Format:** Separeted date format\n",
        "   - **Desired Output Format:** `YYYY-MM-DD` (e.g., `2024-10-04`)\n",
        "\n",
        "### 2. **Concatenation Logic**\n",
        "   - Extract relevant date components:\n",
        "     - Year: `YYYY`\n",
        "     - Month: `MM`\n",
        "     - Day: `DD`\n",
        "   - Concatenate components into a single string:\n",
        "     ```python\n",
        "     concatenated_date = f\"{year}-{month}-{day}\"\n",
        "     ```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8klVhyEPrAtl"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Concatenate Date\n",
        "def concatenate_flight_date(year, month, day):\n",
        "    try:\n",
        "        year = str(year).zfill(4)\n",
        "        month = str(month).zfill(2)\n",
        "        day = str(day).zfill(2)\n",
        "        date = f\"{year}-{month}-{day}\"\n",
        "        logger.debug(f\"Concatenated date: {date}\")\n",
        "        return date\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error concatenating date: {e}\")\n",
        "        return None\n",
        "\n",
        "# UDF Functions for Spark\n",
        "concatenate_flight_date_udf = udf(concatenate_flight_date, StringType())\n",
        "\n",
        "def enrich_concatenate_flight_date(df):\n",
        "    df_enriched = df.withColumn('flight_date', concatenate_flight_date_udf(df['year'], df['month'], df['day']))\n",
        "    return df_enriched"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sONWUPX38kd"
      },
      "source": [
        "# **ENRICHMENT:** Getting Airport Latitude and Longitude by using Airport DB API\n",
        "This section details the process of enriching data by retrieving airport latitude and longitude from the Airport DB API, which will be utilized in Google Colab for further analysis or visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pXbFDle7Qhso"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "api_token = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
        "\n",
        "# Call External API AirportDB\n",
        "def get_airport_coordinates(airport_code):\n",
        "\n",
        "    url = f\"https://airportdb.io/api/v1/airport/{airport_code}?apiToken={api_token}\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        latitude = data.get(\"latitude_deg\")\n",
        "        longitude = data.get(\"longitude_deg\")\n",
        "        logger.debug(f\"Coordinates for {airport_code}: ({latitude}, {longitude})\")\n",
        "        return latitude, longitude\n",
        "    except requests.RequestException as e:\n",
        "        logger.error(f\"Request failed for {airport_code}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# UDF Functions for Spark\n",
        "def get_latitude(airport_code):\n",
        "    lat, lon = get_airport_coordinates(airport_code)\n",
        "    logger.debug(f\"Latitude for {airport_code}: {lat}\")\n",
        "    return lat\n",
        "\n",
        "def get_longitude(airport_code):\n",
        "    lat, lon = get_airport_coordinates(airport_code)\n",
        "    logger.debug(f\"Longitude for {airport_code}: {lon}\")\n",
        "    return lon\n",
        "\n",
        "# Registering UDF Functions\n",
        "get_latitude_udf = udf(get_latitude, FloatType())\n",
        "get_longitude_udf = udf(get_longitude, FloatType())\n",
        "\n",
        "def enrich_with_airport_coordinates_data(df):\n",
        "    df_enriched = df \\\n",
        "        .withColumn('origin_latitude', get_latitude_udf(df['icao_origin'])) \\\n",
        "        .withColumn('origin_longitude', get_longitude_udf(df['icao_origin'])) \\\n",
        "        .withColumn(\"dest_latitude\", get_latitude_udf(df[\"icao_dest\"])) \\\n",
        "        .withColumn(\"dest_longitude\", get_longitude_udf(df[\"icao_dest\"]))\n",
        "    return df_enriched"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JkTRUZG0V9f"
      },
      "source": [
        "# **ENRICHMENT:** Getting Airport Latitude and Longitude by using Python Database airportsdata\n",
        "This section details the process of enriching data by retrieving airport latitude and longitude from the Python Database airportsdata, which will be utilized in Google Colab for further analysis or visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "h8mZTCik0ntb"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import FloatType\n",
        "import airportsdata\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def get_airport_data(airport_code):\n",
        "  airports = airportsdata.load('IATA')  # key is the IATA location code\n",
        "  return airports[airport_code]\n",
        "\n",
        "# UDF Functions for Spark\n",
        "def get_latitude(airport_code):\n",
        "    airport_data = get_airport_data(airport_code)\n",
        "    logger.debug(f\"Latitude for {airport_code}: {airport_data['lat']}\")\n",
        "    return airport_data['lat']\n",
        "\n",
        "def get_longitude(airport_code):\n",
        "    airport_data = get_airport_data(airport_code)\n",
        "    logger.debug(f\"Longitude for {airport_code}: {airport_data['lon']}\")\n",
        "    return airport_data['lon']\n",
        "\n",
        "# Registering UDF Functions\n",
        "get_latitude_udf = udf(get_latitude, FloatType())\n",
        "get_longitude_udf = udf(get_longitude, FloatType())\n",
        "\n",
        "def enrich_with_airport_coordinates_data_database(df):\n",
        "    df_enriched = df \\\n",
        "        .withColumn('origin_latitude', get_latitude_udf(df['origin'])) \\\n",
        "        .withColumn('origin_longitude', get_longitude_udf(df['origin'])) \\\n",
        "        .withColumn(\"dest_latitude\", get_latitude_udf(df[\"dest\"])) \\\n",
        "        .withColumn(\"dest_longitude\", get_longitude_udf(df[\"dest\"]))\n",
        "    return df_enriched"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N7CLaoh4Q_x"
      },
      "source": [
        "# **ENRICHMENT:** Getting Wind Speed by using Weatherbit API\n",
        "This section outlines the process of enriching data by retrieving wind speed from the Weatherbit API, to be utilized in Google Colab for further analysis or visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ba7yMYZAJnst"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "api_key = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
        "\n",
        "# Call External API Weatherbit\n",
        "def get_wind_speed(lat, lon, date):\n",
        "\n",
        "    #url = 'https://api.weatherbit.io/v2.0/history/agweather'\n",
        "    #não foi utilizado dados históricos devido ao princing da ferramenta\n",
        "\n",
        "    url = 'https://api.weatherbit.io/v2.0/current'\n",
        "\n",
        "    params = {\n",
        "        'lat': lat,\n",
        "        'lon': lon,\n",
        "        'key': api_key\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        wind_speed = data['data'][0]['wind_spd']\n",
        "        logger.debug(f\"Wind speed for ({lat}, {lon}) on {date}: {wind_speed}\")\n",
        "        return wind_speed\n",
        "    except requests.RequestException as e:\n",
        "        logger.error(f\"Request failed for ({lat}, {lon}) on {date}: {e}\")\n",
        "        return None\n",
        "\n",
        "# UDF Functions for Spark\n",
        "get_wind_speed_udf = udf(get_wind_speed, FloatType())\n",
        "\n",
        "def enrich_with_weather_data(df):\n",
        "    df_enriched = df \\\n",
        "        .withColumn('wind_speed_origin', get_wind_speed_udf(df['origin_latitude'], df['origin_longitude'], df['flight_date'])) \\\n",
        "        .withColumn('wind_speed_dest', get_wind_speed_udf(df['dest_latitude'], df['dest_longitude'], df['flight_date']))\n",
        "    return df_enriched"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqUSht7zf6_9"
      },
      "source": [
        "# **ENRICHMENT:** Getting random Wind Speed based on Beaufort Wind Scale\n",
        "This section outlines the process of enriching data by retrieving wind speed randomly using Beaufort Wind Scale, to be utilized in Google Colab for further analysis or visualization.\n",
        "\n",
        "- **0 (Calm):** < 1 km/h (0–0.3 m/s)\n",
        "- **1 (Light air):** 1–5 km/h (0.3–1.5 m/s)\n",
        "- **2 (Light breeze):** 6–11 km/h (1.6–3.3 m/s)\n",
        "- **3 (Gentle breeze):** 12–19 km/h (3.4–5.4 m/s)\n",
        "- **4 (Moderate breeze):** 20–28 km/h (5.5–7.9 m/s)\n",
        "- **5 (Fresh breeze):** 29–38 km/h (8.0–10.7 m/s)\n",
        "- **6 (Strong breeze):** 39–49 km/h (10.8–13.8 m/s)\n",
        "- **7 (High wind, moderate gale, near gale):** 50–61 km/h (13.9–17.1 m/s)\n",
        "- **8 (Gale, fresh gale):** 62–74 km/h (17.2–20.7 m/s)\n",
        "- **9 (Strong gale):** 75–88 km/h (20.8–24.4 m/s)\n",
        "- **10 (Storm, whole gale):** 89–102 km/h (24.5–28.4 m/s)\n",
        "- **11 (Violent storm):** 103–117 km/h (28.5–32.6 m/s)\n",
        "- **12 (Hurricane force):** > 118 km/h (>32.7 m/s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EQNNyCJSgTLd"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import FloatType\n",
        "import random\n",
        "\n",
        "def generate_random_beaufort():\n",
        "    # Generate a random floating-point number between 0 and 12 with two decimal places\n",
        "    beaufort_number = round(random.uniform(0, 12), 2)\n",
        "\n",
        "    return beaufort_number\n",
        "\n",
        "# UDF Functions for Spark\n",
        "generate_random_beaufort_udf = udf(generate_random_beaufort, FloatType())\n",
        "\n",
        "def enrich_with_weather_data_random_beaufort(df):\n",
        "    df_enriched = df \\\n",
        "        .withColumn('wind_speed_origin', generate_random_beaufort_udf()) \\\n",
        "        .withColumn('wind_speed_dest', generate_random_beaufort_udf())\n",
        "    return df_enriched"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMEQU7qvkE7f"
      },
      "source": [
        "# **ENRICHMENT:** Transforming Arriving Delay Data in to Labels\n",
        "This section outlines the process of enriching data by transforming arriving delay data in to labels, to be utilized in Google Colab for further analysis or visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DO6Xgbb1kRlO"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "def get_arr_delay_label(arr_delay):\n",
        "    # Handle None values (null in Spark DataFrame)\n",
        "    if arr_delay is None:\n",
        "        return 0\n",
        "    elif arr_delay <= 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# UDF Functions for Spark\n",
        "get_arr_delay_label_udf = udf(get_arr_delay_label, IntegerType())\n",
        "\n",
        "def enrich_with_arr_delay_label(df):\n",
        "    df_enriched = df \\\n",
        "        .withColumn('arr_delay_label', get_arr_delay_label_udf(df['arr_delay']))\n",
        "    return df_enriched"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGYaEVIImYLZ"
      },
      "source": [
        "# **ENRICHMENT:** Transforming Departure Delay Data in to Labels\n",
        "This section outlines the process of enriching data by transforming departure delay data in to labels, to be utilized in Google Colab for further analysis or visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XJoNLDjXmdhV"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "import random\n",
        "\n",
        "def get_dep_delay_label(dep_delay):\n",
        "    # Handle None values (null in Spark DataFrame)\n",
        "    if dep_delay is None:\n",
        "        return 0\n",
        "    elif dep_delay <= 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# UDF Functions for Spark\n",
        "get_dep_delay_label_udf = udf(get_dep_delay_label, IntegerType())\n",
        "\n",
        "def enrich_with_dep_delay_label(df):\n",
        "    df_enriched = df \\\n",
        "        .withColumn('dep_delay_label', get_dep_delay_label_udf(df['dep_delay']))\n",
        "    return df_enriched"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV0vtNMI5DUS"
      },
      "source": [
        "# Main Enrichment Job\n",
        "Details the main enrichment job of the ETL process, focusing on enhancing the dataset by integrating external data sources and performing necessary transformations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld-2mVIpMQ4E",
        "outputId": "836667a9-a920-4283-cd57-9cd1897e083d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+\n",
            "| id|year|month|day|dep_time|sched_dep_time|dep_delay|arr_time|sched_arr_time|arr_delay|carrier|flight|tailnum|origin|dest|air_time|distance|hour|minute|          time_hour|                name|\n",
            "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+\n",
            "|  0|2013|    1|  1|   517.0|           515|      2.0|   830.0|           819|     11.0|     UA|  1545| N14228|   EWR| IAH|   227.0|    1400|   5|    15|2013-01-01 05:00:00|United Air Lines ...|\n",
            "|  1|2013|    1|  1|   533.0|           529|      4.0|   850.0|           830|     20.0|     UA|  1714| N24211|   LGA| IAH|   227.0|    1416|   5|    29|2013-01-01 05:00:00|United Air Lines ...|\n",
            "|  2|2013|    1|  1|   542.0|           540|      2.0|   923.0|           850|     33.0|     AA|  1141| N619AA|   JFK| MIA|   160.0|    1089|   5|    40|2013-01-01 05:00:00|American Airlines...|\n",
            "|  3|2013|    1|  1|   544.0|           545|     -1.0|  1004.0|          1022|    -18.0|     B6|   725| N804JB|   JFK| BQN|   183.0|    1576|   5|    45|2013-01-01 05:00:00|     JetBlue Airways|\n",
            "|  4|2013|    1|  1|   554.0|           600|     -6.0|   812.0|           837|    -25.0|     DL|   461| N668DN|   LGA| ATL|   116.0|     762|   6|     0|2013-01-01 06:00:00|Delta Air Lines Inc.|\n",
            "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+-----------+---------+-----------+---------+-------+---------------+----------------+-------------+--------------+---------------+---------------+-----------------+---------------+\n",
            "| id|year|month|day|dep_time|sched_dep_time|dep_delay|arr_time|sched_arr_time|arr_delay|carrier|flight|tailnum|origin|dest|air_time|distance|hour|minute|          time_hour|                name|icao_origin|icao_dest|flight_date|week_date|  route|origin_latitude|origin_longitude|dest_latitude|dest_longitude|arr_delay_label|dep_delay_label|wind_speed_origin|wind_speed_dest|\n",
            "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+-----------+---------+-----------+---------+-------+---------------+----------------+-------------+--------------+---------------+---------------+-----------------+---------------+\n",
            "|  0|2013|    1|  1|   517.0|           515|      2.0|   830.0|           819|     11.0|     UA|  1545| N14228|   EWR| IAH|   227.0|    1400|   5|    15|2013-01-01 05:00:00|United Air Lines ...|       KEWR|     KIAH| 2013-01-01|  Tuesday|EWR-IAH|       40.69248|       -74.16869|     29.98444|     -95.34144|              0|              0|            11.63|          11.63|\n",
            "|  1|2013|    1|  1|   533.0|           529|      4.0|   850.0|           830|     20.0|     UA|  1714| N24211|   LGA| IAH|   227.0|    1416|   5|    29|2013-01-01 05:00:00|United Air Lines ...|       KLGA|     KIAH| 2013-01-01|  Tuesday|LGA-IAH|       40.77725|       -73.87261|     29.98444|     -95.34144|              0|              0|             2.55|           2.55|\n",
            "|  2|2013|    1|  1|   542.0|           540|      2.0|   923.0|           850|     33.0|     AA|  1141| N619AA|   JFK| MIA|   160.0|    1089|   5|    40|2013-01-01 05:00:00|American Airlines...|       KJFK|     KMIA| 2013-01-01|  Tuesday|JFK-MIA|       40.63993|       -73.77869|     25.79536|     -80.29012|              0|              0|             0.61|           0.61|\n",
            "|  3|2013|    1|  1|   544.0|           545|     -1.0|  1004.0|          1022|    -18.0|     B6|   725| N804JB|   JFK| BQN|   183.0|    1576|   5|    45|2013-01-01 05:00:00|     JetBlue Airways|       KJFK|     TJBQ| 2013-01-01|  Tuesday|JFK-BQN|       40.63993|       -73.77869|     18.49486|     -67.12944|              1|              1|             2.63|           2.63|\n",
            "|  4|2013|    1|  1|   554.0|           600|     -6.0|   812.0|           837|    -25.0|     DL|   461| N668DN|   LGA| ATL|   116.0|     762|   6|     0|2013-01-01 06:00:00|Delta Air Lines Inc.|       KLGA|     KATL| 2013-01-01|  Tuesday|LGA-ATL|       40.77725|       -73.87261|      33.6367|     -84.42786|              1|              1|             9.69|           9.69|\n",
            "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+-----------+---------+-----------+---------+-------+---------------+----------------+-------------+--------------+---------------+---------------+-----------------+---------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, sum, concat_ws\n",
        "\n",
        "#ingesting data\n",
        "df_flights = ingest_fligths_data('drive/MyDrive/code/case-machine-learning-engineer-pleno/notebook/airports-database.csv')\n",
        "\n",
        "#enriching getting airport coordinates data on AirportDB API\n",
        "#Not used due princing and API's limitation\n",
        "#df_flights_enriched = enrich_with_airport_coordinates_data(df_flights_enriched)\n",
        "\n",
        "#enriching getting weather/wind data on Weatherbit API\n",
        "#Not used due princing and API's limitation\n",
        "#df_flights_enriched = enrich_with_weather_data(df_flights_enriched)\n",
        "\n",
        "\n",
        "#enriching getting ICAO airport code\n",
        "df_flights_enriched = enrich_with_airport_icao_data(df_flights)\n",
        "\n",
        "#enriching concatenating flight date\n",
        "df_flights_enriched = enrich_concatenate_flight_date(df_flights_enriched)\n",
        "\n",
        "#enriching getting week date\n",
        "df_flights_enriched = enrich_get_week_date(df_flights_enriched)\n",
        "\n",
        "#enriching concatenating flights as route\n",
        "df_flights_enriched = df_flights_enriched.withColumn(\"route\", concat_ws(\"-\", df_flights_enriched.origin, df_flights_enriched.dest))\n",
        "\n",
        "#enriching getting airport coordinates data on Python Database\n",
        "df_flights_enriched = enrich_with_airport_coordinates_data_database(df_flights_enriched)\n",
        "\n",
        "#enriching getting Departure Delay Label\n",
        "df_flights_enriched = enrich_with_arr_delay_label(df_flights_enriched)\n",
        "\n",
        "#enriching getting Arrive Delay Label\n",
        "df_flights_enriched = enrich_with_dep_delay_label(df_flights_enriched)\n",
        "\n",
        "#enriching getting Wind Speed based on random\n",
        "df_flights_enriched = enrich_with_weather_data_random_beaufort(df_flights_enriched)\n",
        "\n",
        "df_flights_enriched.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11bVz_ha5Q8e"
      },
      "source": [
        "# Processing Output Job\n",
        "This section details the processing output job of the ETL pipeline, focusing on preparing and exporting the enriched data for further analysis or storage and answering all the questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOoHAYamsjjn"
      },
      "source": [
        "<h1> <b> 1. Qual é o número total de voos no conjunto de dados? </b> </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nySCDV0zs9zN",
        "outputId": "2daee1ad-1843-493d-d6a8-fdbda8c1d028"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total flights: 336776\n"
          ]
        }
      ],
      "source": [
        "total_flights = df_flights_enriched.count()\n",
        "print(f\"Total flights: {total_flights}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyO6eFUKtMOM"
      },
      "source": [
        "<h1> <b>2. Quantos voos foram cancelados? (Considerando que voos cancelados têm dep_time e arr_time nulos) </b> </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhM_2EbutLne",
        "outputId": "1e8e59d8-1f7a-4be7-8e59-203e0d031db1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cancelled Flights: 8255\n"
          ]
        }
      ],
      "source": [
        "cancelled_flights = df_flights_enriched.filter((df_flights_enriched.dep_time.isNull()) & (df_flights_enriched.arr_time.isNull())).count()\n",
        "print(f\"Cancelled Flights: {cancelled_flights}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpMQIJ3TuEBB"
      },
      "source": [
        "<h1> <b>3. Qual é o atraso médio na partida dos voos (dep_delay)? </b> </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2jKDRVLuJmO",
        "outputId": "b373085e-2577-48e7-ef2c-b4e4528eff5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Departure Delay: 12.639070257304708 minutes\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import avg\n",
        "\n",
        "avg_dep_delay = df_flights_enriched.agg(avg(\"dep_delay\")).first()[0]\n",
        "print(f\"Average Departure Delay: {avg_dep_delay} minutes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vEc09F-up9P"
      },
      "source": [
        "<h1> <b>4. Quais são os 5 aeroportos com maior número de pousos? </b> </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q48bRWAxuulT",
        "outputId": "38ec6764-6bd8-4c5d-ce3b-c8c1eb85c3e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+-----+\n",
            "|dest|count|\n",
            "+----+-----+\n",
            "| ORD|17283|\n",
            "| ATL|17215|\n",
            "| LAX|16174|\n",
            "| BOS|15508|\n",
            "| MCO|14082|\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import desc\n",
        "\n",
        "top_5_airports = df_flights_enriched.groupBy(\"dest\").count().orderBy(desc(\"count\")).limit(5)\n",
        "top_5_airports.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrg0vglqu9sl"
      },
      "source": [
        "<h1> <b>5. Qual é a rota mais frequente (par origin-dest)? </b> </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DONEQ2iru5tb",
        "outputId": "284d61af-ed05-42a0-873c-9a11063dd3bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----+\n",
            "|  route|count|\n",
            "+-------+-----+\n",
            "|JFK-LAX|11262|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "most_frequent_route = df_flights_enriched.groupBy(\"route\").count().orderBy(desc(\"count\")).limit(1)\n",
        "most_frequent_route.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc1RcuxdvPZS"
      },
      "source": [
        "<h1> <b>6. Quais são as 5 companhias aéreas com maior tempo médio de atraso na chegada? (Exiba também o tempo) </b> </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCsh4sC2vdEJ",
        "outputId": "82da7f2c-7a3b-4c15-cd5a-0ce026e0856a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------------+\n",
            "|carrier|     avg_arr_delay|\n",
            "+-------+------------------+\n",
            "|     F9|21.920704845814978|\n",
            "|     FL|20.115905511811025|\n",
            "|     EV| 15.79643108710965|\n",
            "|     YV|15.556985294117647|\n",
            "|     OO|11.931034482758621|\n",
            "+-------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "top_5_carriers = df_flights_enriched.groupBy(\"carrier\").agg(avg(\"arr_delay\").alias(\"avg_arr_delay\")).orderBy(desc(\"avg_arr_delay\")).limit(5)\n",
        "top_5_carriers.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txVYU25Jvv4a"
      },
      "source": [
        "<h1> <b>7. Qual é o dia da semana com maior número de voos? </b> </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tyz1gLVfvnS_",
        "outputId": "3e701854-0e27-4e90-bf76-cdee58f550e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+-----+\n",
            "|week_date|count|\n",
            "+---------+-----+\n",
            "|   Monday|50690|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "top_day_of_week = df_flights_enriched.groupBy(\"week_date\").count().orderBy(desc(\"count\")).limit(1)\n",
        "top_day_of_week.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd9Ll31FwMDr"
      },
      "source": [
        "<h1> <b>8. Qual o percentual mensal dos voos que tiveram atraso na partida superior a 30 minutos? </b> </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_C5JA3XwQwY",
        "outputId": "38499224-20f6-4f2a-9414-7bff3125c280"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-------------------+\n",
            "|month|    percent_delayed|\n",
            "+-----+-------------------+\n",
            "|   12|0.17967539653264478|\n",
            "|    1|0.12649624287278632|\n",
            "|    3|0.15404139706145212|\n",
            "|    4|0.16379871303593377|\n",
            "|   10|0.09412626950057586|\n",
            "|   11|0.08832994266691326|\n",
            "|    2|0.13431827775432673|\n",
            "|    6|0.20992142175222148|\n",
            "|    5|0.15641270853256828|\n",
            "|    9|0.08918958778851117|\n",
            "|    8| 0.1469435872542561|\n",
            "|    7| 0.2167105494119712|\n",
            "+-----+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Criar uma coluna \"is_delayed\" como 1 para atrasos > 30 min, e 0 caso contrário\n",
        "df_flights_delays = df_flights_enriched.withColumn(\"is_delayed\", col(\"dep_delay\") > 30)\n",
        "\n",
        "# Converter valores booleanos para inteiros\n",
        "df_flights_delays = df_flights_delays.withColumn(\"is_delayed_int\", col(\"is_delayed\").cast(\"int\"))\n",
        "\n",
        "# Calcular o percentual de voos com atraso na partida maior que 30 minutos por mês\n",
        "monthly_delays = df_flights_delays.groupBy(\"month\").agg((avg(\"is_delayed_int\").alias(\"percent_delayed\")))\n",
        "monthly_delays.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b0KPoJ9-yPy"
      },
      "source": [
        "<h1> <b>9. Qual é a origem mais comum para voos que pousaram em Seattle (SEA)? </b> </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Kic9gHN-7s6",
        "outputId": "3a737dad-1888-4062-f2cc-0ec571565c93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-----+\n",
            "|origin|count|\n",
            "+------+-----+\n",
            "|   JFK| 2092|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "common_origin_for_sea = df_flights_enriched.filter(df_flights_enriched.dest == \"SEA\").groupBy(\"origin\").count().orderBy(desc(\"count\")).limit(1)\n",
        "common_origin_for_sea.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mplz_etM_Ekj"
      },
      "source": [
        "<h1> <b>10. Qual é a média de atraso na partida dos voos (dep_delay) para cada dia da semana? </b> </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_YDinu0_LH5",
        "outputId": "49aa124e-3344-4e27-a3e1-3036b90eb8da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+------------------+\n",
            "|week_date|     avg_dep_delay|\n",
            "+---------+------------------+\n",
            "|Wednesday|11.803512219083876|\n",
            "|  Tuesday|10.631682565455652|\n",
            "|   Friday| 14.69605749486653|\n",
            "| Thursday|16.148919990957108|\n",
            "| Saturday| 7.650502333676133|\n",
            "|   Monday|14.778936729330908|\n",
            "|   Sunday|11.589531801152422|\n",
            "+---------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "avg_delay_by_day_of_week = df_flights_enriched.groupBy(\"week_date\").agg(avg(\"dep_delay\").alias(\"avg_dep_delay\"))\n",
        "avg_delay_by_day_of_week.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ckX78LY_ZFN"
      },
      "source": [
        "<h1> <b>11. Qual é a rota que teve o maior tempo de voo médio (air_time)? </b> </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5gTB4PO_hVR",
        "outputId": "f75b0778-fe47-44a0-cf7d-b4f26ff9eb37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----------------+\n",
            "|  route|     avg_air_time|\n",
            "+-------+-----------------+\n",
            "|JFK-HNL|623.0877192982456|\n",
            "+-------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "longest_avg_air_time_route = df_flights_enriched.groupBy(\"route\").agg(avg(\"air_time\").alias(\"avg_air_time\")).orderBy(desc(\"avg_air_time\")).limit(1)\n",
        "longest_avg_air_time_route.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlFxOayGCRdK"
      },
      "source": [
        "<h1> <b>12. Para cada aeroporto de origem, qual é o aeroporto de destino mais comum? </b> </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNxkOBI2CWql",
        "outputId": "9453e20d-41fe-44f9-ad78-93d5678502ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----+------------+\n",
            "|origin|dest|flight_count|\n",
            "+------+----+------------+\n",
            "|   EWR| ORD|        6100|\n",
            "|   EWR| BOS|        5327|\n",
            "|   EWR| SFO|        5127|\n",
            "|   EWR| CLT|        5026|\n",
            "|   EWR| ATL|        5022|\n",
            "|   EWR| MCO|        4941|\n",
            "|   EWR| LAX|        4912|\n",
            "|   EWR| IAH|        3973|\n",
            "|   EWR| FLL|        3793|\n",
            "|   EWR| DTW|        3178|\n",
            "|   EWR| DFW|        3148|\n",
            "|   EWR| DEN|        2859|\n",
            "|   EWR| PHX|        2723|\n",
            "|   EWR| CVG|        2673|\n",
            "|   EWR| MIA|        2633|\n",
            "|   EWR| STL|        2516|\n",
            "|   EWR| MSP|        2377|\n",
            "|   EWR| PBI|        2351|\n",
            "|   EWR| BNA|        2336|\n",
            "|   EWR| TPA|        2334|\n",
            "+------+----+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "most_common_dest_per_origin = df_flights_enriched.groupBy(\"origin\", \"dest\").count().withColumnRenamed(\"count\", \"flight_count\").orderBy(\"origin\", desc(\"flight_count\"))\n",
        "most_common_dest_per_origin.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzSK-5RMCc_S"
      },
      "source": [
        "<h1> <b>13. Quais são as 3 rotas que tiveram a maior variação no tempo médio de voo (air_time)? </b> </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNXLz4d8Clgo",
        "outputId": "98251815-72bc-4a65-e3a1-466db77969a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------------+\n",
            "|  route|      std_air_time|\n",
            "+-------+------------------+\n",
            "|LGA-MYR| 25.32455988429677|\n",
            "|EWR-HNL| 21.26613546847427|\n",
            "|JFK-HNL|20.688824842787056|\n",
            "+-------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import stddev\n",
        "\n",
        "routes_with_highest_variation = df_flights_enriched.groupBy(\"route\").agg(stddev(\"air_time\").alias(\"std_air_time\")).orderBy(desc(\"std_air_time\")).limit(3)\n",
        "routes_with_highest_variation.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxaMW99vCrmM"
      },
      "source": [
        "<h1> <b>14. Qual é a média de atraso na chegada para voos que tiveram atraso na partida superior a 1 hora? </b> </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "az-wKBA-CyJn",
        "outputId": "e10fc5ec-254f-44f7-fce5-cbe833cf2fbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+\n",
            "|     avg_arr_delay|\n",
            "+------------------+\n",
            "|119.04880549963919|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "avg_arr_delay_for_long_dep_delays = df_flights_enriched.filter(df_flights_enriched.dep_delay > 60).agg(avg(\"arr_delay\").alias(\"avg_arr_delay\"))\n",
        "avg_arr_delay_for_long_dep_delays.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy-qPeGGDcod"
      },
      "source": [
        "<h1> <b>15. Qual é a média de voos diários para cada mês do ano? </b> </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZTToCiXDzbg",
        "outputId": "20f4d093-0a3e-4529-9ddd-378068108c02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-----------------+\n",
            "|month|avg_daily_flights|\n",
            "+-----+-----------------+\n",
            "|   12|907.5806451612904|\n",
            "|    1|871.0967741935484|\n",
            "|    6|941.4333333333333|\n",
            "|    3|930.1290322580645|\n",
            "|    5|928.9032258064516|\n",
            "|    9|919.1333333333333|\n",
            "|    4|944.3333333333334|\n",
            "|    8|946.0322580645161|\n",
            "|    7|949.1935483870968|\n",
            "|   10|931.9032258064516|\n",
            "|   11|908.9333333333333|\n",
            "|    2|891.1071428571429|\n",
            "+-----+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "daily_avg_flights_per_month = df_flights_enriched.groupBy(\"month\", \"day\").count().groupBy(\"month\").agg(avg(\"count\").alias(\"avg_daily_flights\"))\n",
        "daily_avg_flights_per_month.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h42jaICKDgGQ"
      },
      "source": [
        "<h1> <b>16. Quais são as 3 rotas mais comuns que tiveram atrasos na chegada superiores a 30 minutos? </b> </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmKnbqKGD2jh",
        "outputId": "c879213f-9cfc-42be-d97e-b8fa68e3f017"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----+\n",
            "|  route|count|\n",
            "+-------+-----+\n",
            "|LGA-ATL| 1563|\n",
            "|JFK-LAX| 1286|\n",
            "|LGA-ORD| 1188|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "most_common_routes_with_long_arrival_delay = df_flights_enriched.filter(df_flights_enriched.arr_delay > 30).groupBy(\"route\").count().orderBy(desc(\"count\")).limit(3)\n",
        "most_common_routes_with_long_arrival_delay.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXEH--PwDrCd"
      },
      "source": [
        "<h1> <b>17. Para cada origem, qual o principal destino? </b> </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4I_-7Z7KEA1y",
        "outputId": "1c570159-59fb-4430-c4c5-a6795243097c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----+\n",
            "|origin|dest|\n",
            "+------+----+\n",
            "|   LGA| ATL|\n",
            "|   EWR| ORD|\n",
            "|   JFK| LAX|\n",
            "+------+----+\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DataFrame[origin: string, dest: string, flight_count: bigint]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.sql import Window\n",
        "from pyspark.sql.functions import col, count, row_number\n",
        "\n",
        "# Repartition the DataFrame based on the \"origin\" column to avoid skew\n",
        "df_flight_counts = df_flights_enriched.repartition(\"origin\").groupBy(\"origin\", \"dest\").agg(count(\"*\").alias(\"flight_count\"))\n",
        "\n",
        "# Define a window partitioned by 'origin', ordering by flight count (most frequent destinations first)\n",
        "window = Window.partitionBy(\"origin\").orderBy(col(\"flight_count\").desc())\n",
        "\n",
        "# Cache the intermediate DataFrame to avoid recomputation\n",
        "df_flight_counts.cache()\n",
        "\n",
        "# Use row_number() to rank destinations for each origin and select the most frequent destination (rank 1)\n",
        "df_top_destinations = df_flight_counts.withColumn(\"row\", row_number().over(window)) \\\n",
        "                                      .filter(col(\"row\") == 1) \\\n",
        "                                      .select(\"origin\", \"dest\")\n",
        "\n",
        "# Show the most frequent destination for each origin\n",
        "df_top_destinations.show(5)\n",
        "df_flight_counts.unpersist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lVqFQazra-5"
      },
      "source": [
        "<h1> <b>Pergunta final:</b> Enriqueça a base de dados de voos com as condições meteorológicas\n",
        "(velocidade do vento) para os aeroportos de origem e destino. Mostre as informações\n",
        "enriquecidas para os 5 voos com maior atraso na chegada.</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-mEaMgBrkBU",
        "outputId": "fc902aac-9052-4919-8e4e-1d7bbfd0319f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----+---------+-----------------+---------------+\n",
            "|origin|dest|arr_delay|wind_speed_origin|wind_speed_dest|\n",
            "+------+----+---------+-----------------+---------------+\n",
            "|   JFK| HNL|   1272.0|             0.93|           0.93|\n",
            "|   JFK| CMH|   1127.0|             2.61|           2.61|\n",
            "|   EWR| ORD|   1109.0|             2.91|           2.91|\n",
            "|   JFK| SFO|   1007.0|             4.87|           4.87|\n",
            "|   JFK| CVG|    989.0|             4.58|           4.58|\n",
            "+------+----+---------+-----------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Selecting the top 5 flights with the largest arrival delay\n",
        "df_top_5_arr_delay = df_flights_enriched.orderBy(col(\"arr_delay\").desc()).limit(5)\n",
        "\n",
        "# Display the information for the top 5 flights with the largest arrival delays, including wind speeds\n",
        "df_top_5_arr_delay.select(\"origin\", \"dest\", \"arr_delay\", \"wind_speed_origin\", \"wind_speed_dest\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gpggi1nAtUQR"
      },
      "source": [
        "<h1> <b>Machine Learning Modeling Phase for Flight Delay Prediction</b> </h1>\n",
        "In this phase, we will focus on the <b>machine learning modeling</b> process for predicting flight delays. The goal is to create a model that can predict the <b>arrival delay</b> based on various features from the flight dataset, including the flight's departure delay, weather conditions (such as wind speed), and other relevant factors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMF8qVpm8GIQ"
      },
      "source": [
        "# Remove rows where any column has a null value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "au9IdFd28G8J",
        "outputId": "974fbd5b-4bdb-4c54-a3d5-9c7214501885"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+-----------+---------+-----------+---------+-------+---------------+----------------+-------------+--------------+---------------+---------------+-----------------+---------------+\n",
            "| id|year|month|day|dep_time|sched_dep_time|dep_delay|arr_time|sched_arr_time|arr_delay|carrier|flight|tailnum|origin|dest|air_time|distance|hour|minute|          time_hour|                name|icao_origin|icao_dest|flight_date|week_date|  route|origin_latitude|origin_longitude|dest_latitude|dest_longitude|arr_delay_label|dep_delay_label|wind_speed_origin|wind_speed_dest|\n",
            "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+-----------+---------+-----------+---------+-------+---------------+----------------+-------------+--------------+---------------+---------------+-----------------+---------------+\n",
            "|  0|2013|    1|  1|   517.0|           515|      2.0|   830.0|           819|     11.0|     UA|  1545| N14228|   EWR| IAH|   227.0|    1400|   5|    15|2013-01-01 05:00:00|United Air Lines ...|       KEWR|     KIAH| 2013-01-01|  Tuesday|EWR-IAH|       40.69248|       -74.16869|     29.98444|     -95.34144|              0|              0|             2.89|           2.89|\n",
            "|  1|2013|    1|  1|   533.0|           529|      4.0|   850.0|           830|     20.0|     UA|  1714| N24211|   LGA| IAH|   227.0|    1416|   5|    29|2013-01-01 05:00:00|United Air Lines ...|       KLGA|     KIAH| 2013-01-01|  Tuesday|LGA-IAH|       40.77725|       -73.87261|     29.98444|     -95.34144|              0|              0|             0.85|           0.85|\n",
            "|  2|2013|    1|  1|   542.0|           540|      2.0|   923.0|           850|     33.0|     AA|  1141| N619AA|   JFK| MIA|   160.0|    1089|   5|    40|2013-01-01 05:00:00|American Airlines...|       KJFK|     KMIA| 2013-01-01|  Tuesday|JFK-MIA|       40.63993|       -73.77869|     25.79536|     -80.29012|              0|              0|             2.79|           2.79|\n",
            "|  3|2013|    1|  1|   544.0|           545|     -1.0|  1004.0|          1022|    -18.0|     B6|   725| N804JB|   JFK| BQN|   183.0|    1576|   5|    45|2013-01-01 05:00:00|     JetBlue Airways|       KJFK|     TJBQ| 2013-01-01|  Tuesday|JFK-BQN|       40.63993|       -73.77869|     18.49486|     -67.12944|              1|              1|             5.37|           5.37|\n",
            "|  4|2013|    1|  1|   554.0|           600|     -6.0|   812.0|           837|    -25.0|     DL|   461| N668DN|   LGA| ATL|   116.0|     762|   6|     0|2013-01-01 06:00:00|Delta Air Lines Inc.|       KLGA|     KATL| 2013-01-01|  Tuesday|LGA-ATL|       40.77725|       -73.87261|      33.6367|     -84.42786|              1|              1|             9.28|           9.28|\n",
            "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+-----------+---------+-----------+---------+-------+---------------+----------------+-------------+--------------+---------------+---------------+-----------------+---------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Remove rows where any column has a null value\n",
        "df_flights_enriched = df_flights_enriched.filter(df_flights_enriched['dep_delay'].isNotNull() & df_flights_enriched['distance'].isNotNull())\n",
        "\n",
        "df_flights_enriched.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHR95JjxuvLh"
      },
      "source": [
        "# **Feature Engineering:** Standardizing the Features\n",
        "Standardize the features before applying PCA. This is done using StandardScaler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "vf9UFN1RKWRU"
      },
      "outputs": [],
      "source": [
        "#df_sampled = df_flights_enriched.sample(fraction=0.0001, seed=42).repartition(200)\n",
        "\n",
        "#row_count = df_sampled.count()\n",
        "\n",
        "# Print the row count\n",
        "#print(f\"Total number of rows: {row_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "WHAcb206vBal"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "#from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "\n",
        "# Sample the data (adjust the fraction as needed)\n",
        "#df_sampled = df_flights_enriched.sample(fraction=0.001, seed=42)\n",
        "\n",
        "# Create the feature vector\n",
        "#assembler = VectorAssembler(inputCols=[\"dep_delay_label\", \"wind_speed_origin\", \"wind_speed_dest\", \"distance\", \"arr_delay_label\"],\n",
        "                           # outputCol=\"features_vector\")\n",
        "#df_flights_vector = assembler.transform(df_sampled)\n",
        "\n",
        "# Standardize the features\n",
        "#scaler = StandardScaler(inputCol=\"features_vector\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
        "#scaler_model = scaler.fit(df_flights_vector)\n",
        "#df_scaled = scaler_model.transform(df_flights_vector)\n",
        "\n",
        "# Cache the DataFrame if needed\n",
        "#df_scaled.cache()\n",
        "\n",
        "# Show the results\n",
        "#df_scaled.select(\"features_vector\", \"scaled_features\").show(5)\n",
        "\n",
        "# Unpersist the DataFrame when no longer needed\n",
        "#df_scaled.unpersist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlvTDV9cviDB"
      },
      "source": [
        "# **Feature Engineering:** Applying PCA (Principal Component Analysis)\n",
        "Once the data is standardized, PCA has been applied to reduce the number of features while keeping most of the variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "G3P25BY2v-sn"
      },
      "outputs": [],
      "source": [
        "#from pyspark.ml.feature import PCA\n",
        "\n",
        "# Initialize PCA model to reduce to 2 principal components\n",
        "#pca = PCA(k=2, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
        "\n",
        "# Fit the PCA model and transform the data (repartition to optimize PCA)\n",
        "#df_pca = pca.fit(df_scaled.repartition(100)).transform(df_scaled)\n",
        "\n",
        "# Cache the result of PCA transformation to reuse if needed\n",
        "#df_pca.cache()\n",
        "\n",
        "# Show the resulting principal components\n",
        "#df_pca.select(\"pca_features\").show(5, truncate=False)\n",
        "\n",
        "#df_pca.unpersist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZnpMm4uoPNZ"
      },
      "source": [
        "# **Modelling:** Spliting Data\n",
        "\n",
        "In the **Modelling: Splitting Data** phase, we divide our dataset into two main parts: **training** and **testing** data. This is a critical step in machine learning because it helps evaluate how well a model generalizes to unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ujs_OQGmoO0T"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "\n",
        "# Combine features and target in one DataFrame\n",
        "df_combined = df_flights_enriched.select(\"dep_delay_label\", \"wind_speed_origin\", \"wind_speed_dest\", \"distance\", \"route\", \"arr_delay_label\")\n",
        "\n",
        "# Split the combined DataFrame into training and testing sets (80% train, 20% test)\n",
        "train_df, test_df = df_combined.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Separate the features and target for the training set\n",
        "X_train = train_df.select(\"dep_delay_label\", \"wind_speed_origin\", \"wind_speed_dest\", \"distance\", \"route\")\n",
        "Y_train = train_df.select(\"arr_delay_label\")\n",
        "\n",
        "# Separate the features and target for the testing set\n",
        "X_test = test_df.select(\"dep_delay_label\", \"wind_speed_origin\", \"wind_speed_dest\", \"distance\", \"route\")\n",
        "Y_test = test_df.select(\"arr_delay_label\")\n",
        "\n",
        "# Index the 'route' column (convert from string to numeric)\n",
        "indexer = StringIndexer(inputCol=\"route\", outputCol=\"route_index\")\n",
        "\n",
        "df_flights_indexed = indexer.fit(df_flights_enriched).transform(df_flights_enriched)\n",
        "\n",
        "# Assemble the features into a single vector (including the indexed 'route')\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"dep_delay_label\", \"wind_speed_origin\", \"wind_speed_dest\", \"distance\", \"route_index\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "# Apply the assembler to create a new DataFrame with features vector\n",
        "df_combined = assembler.transform(df_flights_indexed)\n",
        "\n",
        "# Select the relevant columns (features and target)\n",
        "df_combined = df_combined.select(\"features\", \"arr_delay_label\")\n",
        "\n",
        "# Split the data into training and testing sets (80% training, 20% testing)\n",
        "train_df, test_df = df_combined.randomSplit([0.8, 0.2], seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4smIzIIEoUR1"
      },
      "source": [
        "# **Modelling:** Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "GYE7oY9noT-X"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "# Initialize the Linear Regression model\n",
        "lr = LinearRegression(featuresCol='features', labelCol='arr_delay_label')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR8ejifX3aj0"
      },
      "source": [
        "# **Modelling:** Model Training & Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "0Xojr46M3ixQ"
      },
      "outputs": [],
      "source": [
        "# Train the model on the training data\n",
        "lr_model = lr.fit(train_df)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = lr_model.transform(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVtEN1IR3niD"
      },
      "source": [
        "# **Modelling:** Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv2GzE4v3sJB",
        "outputId": "b4e64500-bd53-452d-8488-d476ec593b95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Root Mean Squared Error (RMSE): 0.4205340977626651\n",
            "R-squared (R2): 0.2649934442865589\n",
            "Coefficients: [0.520025812633214,-2.7340289199983374e-05,-2.7340289199983374e-05,2.9393253407518075e-05,-4.274369780951944e-05]\n",
            "Intercept: 0.24646743386366174\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Evaluate the model using RMSE and R-squared\n",
        "evaluator_rmse = RegressionEvaluator(labelCol=\"arr_delay_label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator_rmse.evaluate(predictions)\n",
        "\n",
        "evaluator_r2 = RegressionEvaluator(labelCol=\"arr_delay_label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "r2 = evaluator_r2.evaluate(predictions)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "print(f\"R-squared (R2): {r2}\")\n",
        "\n",
        "# Optionally, print the model coefficients and intercept\n",
        "print(f\"Coefficients: {lr_model.coefficients}\")\n",
        "print(f\"Intercept: {lr_model.intercept}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsRZXVZ9oyBs"
      },
      "source": [
        "# **Modelling:** Exporting Pickle file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXPDIR7wpGuI",
        "outputId": "532d5fc4-df2f-493a-872e-2b44eaea152c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model data has been saved to 'linear_regression_model_flight_delay_prediction.pkl'\n"
          ]
        }
      ],
      "source": [
        "# Extract the coefficients and intercept of the Linear Regression model\n",
        "model_data = {\n",
        "    \"coefficients\": lr_model.coefficients.toArray(),  # Convert to array\n",
        "    \"intercept\": lr_model.intercept\n",
        "}\n",
        "\n",
        "# Save the model data to a .pkl file\n",
        "import pickle\n",
        "with open('linear_regression_model_flight_delay_prediction.pkl', 'wb') as f:\n",
        "    pickle.dump(model_data, f)\n",
        "\n",
        "print(\"Model data has been saved to 'linear_regression_model_flight_delay_prediction.pkl'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRJ-wkuIGF-z"
      },
      "source": [
        "# **Modelling:** Testing the model .pkl file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74vM4zU6stbn",
        "outputId": "a51bd375-76b9-4495-a55a-31c7763e77c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: 0.5092931676916015\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Load the model data from the .pkl file\n",
        "with open('linear_regression_model_flight_delay_prediction.pkl', 'rb') as f:\n",
        "    model_data = pickle.load(f)\n",
        "\n",
        "# Function to manually apply the linear regression formula: y = X * coefficients + intercept\n",
        "def manual_predict(features, coefficients, intercept):\n",
        "    return np.dot(features, coefficients) + intercept\n",
        "\n",
        "# Example: Making a prediction with a sample feature vector\n",
        "sample_features = np.array([0.5, 1.2, 0.3, 100, 2])\n",
        "\n",
        "prediction = manual_predict(sample_features, model_data[\"coefficients\"], model_data[\"intercept\"])\n",
        "print(f\"Prediction: {prediction}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
